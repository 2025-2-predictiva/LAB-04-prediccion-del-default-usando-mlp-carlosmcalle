{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# importamos las librerias necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import gzip\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    precision_score,\n",
        "    balanced_accuracy_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n"
      ],
      "metadata": {
        "id": "7uTJqHKcGHs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones auxiliares\n",
        "\n",
        "def crear_directorios_base():\n",
        "    \"\"\"Asegura que los directorios de salida existan.\"\"\"\n",
        "    os.makedirs(\"../files/models\", exist_ok=True)\n",
        "    os.makedirs(\"../files/output\", exist_ok=True)\n",
        "\n",
        "def cargar_datos_fuente():\n",
        "    \"\"\"Carga los DataFrames de entrenamiento y prueba.\"\"\"\n",
        "    train_df = pd.read_csv(\"../files/input/train_data.csv.zip\")\n",
        "    test_df = pd.read_csv(\"../files/input/test_data.csv.zip\")\n",
        "    return train_df, test_df\n",
        "\n",
        "def guardar_modelo_comprimido(estimator, path=\"../files/models/model.pkl.gz\"):\n",
        "    \"\"\"Guarda el estimador comprimido con gzip.\"\"\"\n",
        "    crear_directorios_base()\n",
        "    with gzip.open(path, \"wb\") as f:\n",
        "        pickle.dump(estimator, f)\n",
        "\n",
        "def recuperar_modelo(path=\"../files/models/model.pkl.gz\"):\n",
        "    \"\"\"Carga el estimador guardado si existe, sino devuelve None.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with gzip.open(path, \"rb\") as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "eAzWvyf2GS5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones para limpiar y cargar los datos\n",
        "\n",
        "def limpiar_dataframe(df):\n",
        "    \"\"\"Limpia y transforma un DataFrame (manejo de columnas y valores atípicos).\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # renombrar target\n",
        "    df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
        "\n",
        "    # eliminar ID\n",
        "    if \"ID\" in df.columns:\n",
        "        df.drop(columns=[\"ID\"], inplace=True)\n",
        "\n",
        "    # eliminar EDUCATION = 0 y agrupar valores > 4 en 4\n",
        "    df = df[df[\"EDUCATION\"] != 0]\n",
        "    df[\"EDUCATION\"] = df[\"EDUCATION\"].apply(lambda x: 4 if x > 4 else x)\n",
        "\n",
        "    # eliminar MARRIAGE = 0\n",
        "    df = df[df[\"MARRIAGE\"] != 0]\n",
        "\n",
        "    # eliminar NaNs\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "1qI3-TcRGcrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# division\n",
        "def obtener_splits_entrenamiento_prueba(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Aplica limpiar_dataframe y retorna x_train, y_train, x_test, y_test\n",
        "    \"\"\"\n",
        "    train_clean = limpiar_dataframe(train_df)\n",
        "    test_clean = limpiar_dataframe(test_df)\n",
        "\n",
        "    x_train = train_clean.drop(columns=[\"default\"])\n",
        "    y_train = train_clean[\"default\"]\n",
        "\n",
        "    x_test = test_clean.drop(columns=[\"default\"])\n",
        "    y_test = test_clean[\"default\"]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "wYbjWCdfGfGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones parael modelo\n",
        "\n",
        "def construir_pipeline_completo(feature_columns):\n",
        "    \"\"\"\n",
        "    Construye el pipeline de preprocesamiento y el modelo MLP.\n",
        "    Orden del Código Fuente: (OHE + StandardScaler) -> SelectKBest -> PCA -> MLP\n",
        "    \"\"\"\n",
        "    categorical_features = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]\n",
        "    # Las demás columnas se consideran numéricas\n",
        "    numeric_features = [c for c in feature_columns if c not in categorical_features]\n",
        "\n",
        "    # ColumnTransformer: OHE para categóricas, StandardScaler para numéricas (como en el Código Fuente)\n",
        "    preprocessor = ColumnTransformer(\n",
        "        [\n",
        "            (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
        "            (\"num\", StandardScaler(), numeric_features),\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            # Orden del Código Fuente: SelectKBest va antes de PCA\n",
        "            (\"feature_selection\", SelectKBest(score_func=f_classif)),\n",
        "            (\"pca\", PCA()),\n",
        "            # Hiperparámetros de MLP del Código Fuente\n",
        "            (\"mlp\", MLPClassifier(\n",
        "                max_iter=15000,\n",
        "                random_state=21 # Random state del Código Fuente\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def configurar_busqueda_grid(estimator, param_grid, cv=10):\n",
        "    \"\"\"Crea y configura el objeto GridSearchCV.\"\"\"\n",
        "    # cv=10 como en el Código Fuente (entero, que por defecto es KFold)\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=estimator,\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring=\"balanced_accuracy\",\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "        refit=True\n",
        "    )\n",
        "    return grid_search\n"
      ],
      "metadata": {
        "id": "nXQq3ZX8G1IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones de Entrenamiento y Validación\n",
        "\n",
        "def entrenar_y_comparar_modelos(grid_search):\n",
        "    \"\"\"\n",
        "    Entrena grid_search con los datos, compara con modelo guardado (si existe)\n",
        "    usando balanced_accuracy en el conjunto de test; mantiene el mejor.\n",
        "    \"\"\"\n",
        "    train_df, test_df = cargar_datos_fuente()\n",
        "    x_train, y_train, x_test, y_test = obtener_splits_entrenamiento_prueba(train_df, test_df)\n",
        "\n",
        "    # entrenar\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    # cargar modelo guardado (si existe) y comparar en balanced_accuracy sobre test\n",
        "    saved = recuperar_modelo()\n",
        "    current_score = balanced_accuracy_score(y_test, grid_search.predict(x_test))\n",
        "\n",
        "    saved_score = -1.0\n",
        "    if saved is not None:\n",
        "        try:\n",
        "            saved_score = balanced_accuracy_score(y_test, saved.predict(x_test))\n",
        "        except Exception:\n",
        "            saved_score = -1.0 # si el objeto guardado no tiene predict (o fue guardado mal)\n",
        "\n",
        "\n",
        "    if current_score >= saved_score:\n",
        "        # guardar el grid_search (fitted) para revisar cv_results etc.\n",
        "        guardar_modelo_comprimido(grid_search)\n",
        "    else:\n",
        "        # mantener el guardado (no sobreescribimos)\n",
        "        pass\n",
        "\n",
        "def ejecutar_entrenamiento_mlp():\n",
        "    \"\"\"Define la cuadrícula de parámetros para MLP y ejecuta el entrenamiento.\"\"\"\n",
        "    train_df, test_df = cargar_datos_fuente()\n",
        "    x_train, y_train, x_test, y_test = obtener_splits_entrenamiento_prueba(train_df, test_df)\n",
        "\n",
        "    pipeline = construir_pipeline_completo(feature_columns=x_train.columns.tolist())\n",
        "\n",
        "    # Parámetros del Código Fuente\n",
        "    param_grid = {\n",
        "    'feature_selection__k': [20], # numero de columnas a seleccionar (Código Fuente)\n",
        "    'pca__n_components': [None],  # usar todas las componentes (Código Fuente)\n",
        "    'mlp__hidden_layer_sizes': [(50, 30, 40, 60)], # capas y neuronas ocultas (Código Fuente)\n",
        "    'mlp__alpha': [0.26], # regularización L2 (Código Fuente)\n",
        "    'mlp__learning_rate_init': [0.001], # tasa de aprendizaje inicial (Código Fuente)\n",
        "    }\n",
        "\n",
        "    # cv=10 como en el Código Fuente.\n",
        "    gs = configurar_busqueda_grid(estimator=pipeline, param_grid=param_grid, cv=10)\n",
        "    entrenar_y_comparar_modelos(gs)"
      ],
      "metadata": {
        "id": "-f7jCeDxGyOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oBJevQEHG_1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwOiJhnVFpB0"
      },
      "outputs": [],
      "source": [
        "# validacion y métricas\n",
        "\n",
        "def validar_y_generar_metricas():\n",
        "    \"\"\"Carga el modelo final, calcula y guarda todas las métricas en un archivo JSONL.\"\"\"\n",
        "    crear_directorios_base()\n",
        "    train_df, test_df = cargar_datos_fuente()\n",
        "    x_train, y_train, x_test, y_test = obtener_splits_entrenamiento_prueba(train_df, test_df)\n",
        "\n",
        "    # cargar modelo (gzip)\n",
        "    estimator = recuperar_modelo()\n",
        "    if estimator is None:\n",
        "        raise FileNotFoundError(\"No se encontró modelo en files/models/model.pkl.gz\")\n",
        "\n",
        "    # predicciones\n",
        "    y_train_pred = estimator.predict(x_train)\n",
        "    y_test_pred = estimator.predict(x_test)\n",
        "\n",
        "    metrics = []\n",
        "\n",
        "    # Métricas de entrenamiento\n",
        "    train_metrics = {\n",
        "        \"type\": \"metrics\",\n",
        "        \"dataset\": \"train\",\n",
        "        \"precision\": precision_score(y_train, y_train_pred, zero_division=0),\n",
        "        \"balanced_accuracy\": balanced_accuracy_score(y_train, y_train_pred),\n",
        "        \"recall\": recall_score(y_train, y_train_pred, zero_division=0),\n",
        "        \"f1_score\": f1_score(y_train, y_train_pred, zero_division=0),\n",
        "    }\n",
        "    metrics.append(train_metrics)\n",
        "\n",
        "    # Métricas de prueba\n",
        "    test_metrics = {\n",
        "        \"type\": \"metrics\",\n",
        "        \"dataset\": \"test\",\n",
        "        \"precision\": precision_score(y_test, y_test_pred, zero_division=0),\n",
        "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_test_pred),\n",
        "        \"recall\": recall_score(y_test, y_test_pred, zero_division=0),\n",
        "        \"f1_score\": f1_score(y_test, y_test_pred, zero_division=0),\n",
        "    }\n",
        "    metrics.append(test_metrics)\n",
        "\n",
        "    # matriz de confusión train\n",
        "    cm_train = confusion_matrix(y_train, y_train_pred)\n",
        "    cm_train_dict = {\n",
        "        \"type\": \"cm_matrix\",\n",
        "        \"dataset\": \"train\",\n",
        "        \"true_0\": {\"predicted_0\": int(cm_train[0, 0]), \"predicted_1\": int(cm_train[0, 1])},\n",
        "        \"true_1\": {\"predicted_0\": int(cm_train[1, 0]), \"predicted_1\": int(cm_train[1, 1])},\n",
        "    }\n",
        "    metrics.append(cm_train_dict)\n",
        "\n",
        "    # matriz de confusión test\n",
        "    cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "    cm_test_dict = {\n",
        "        \"type\": \"cm_matrix\",\n",
        "        \"dataset\": \"test\",\n",
        "        \"true_0\": {\"predicted_0\": int(cm_test[0, 0]), \"predicted_1\": int(cm_test[0, 1])},\n",
        "        \"true_1\": {\"predicted_0\": int(cm_test[1, 0]), \"predicted_1\": int(cm_test[1, 1])},\n",
        "    }\n",
        "    metrics.append(cm_test_dict)\n",
        "\n",
        "    # guardar JSONL\n",
        "    out_path = \"../files/output/metrics.json\"\n",
        "    with open(out_path, \"w\") as f:\n",
        "        for m in metrics:\n",
        "            f.write(json.dumps(m) + \"\\n\")\n",
        "\n",
        "    print(f\"Métricas guardadas en {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # si se ejecuta el script, entrena y luego comprueba\n",
        "    crear_directorios_base()\n",
        "    ejecutar_entrenamiento_mlp()\n",
        "    validar_y_generar_metricas()"
      ]
    }
  ]
}